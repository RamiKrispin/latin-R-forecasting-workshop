---
title: "Modeling Seasonality"
author: "Rami Krispin"
date: last-modified
format: 
    html:
        code-fold: false
        warning: false
        toc: true
---


```{r}
#| label: setup
library(feasts)
library(fabletools)
library(tsibble)
library(dplyr)
library(plotly)

source("./functions.R")
```

Next, let's load the datasets:
```{r}
#| label: load the data

load(file = "./data/ts.RData")
```



```{r}
p <- plot_ly() |>
    add_lines(x = ts2$index, y = ts2$y, type = "scatter", mode = "lines", name = "Actual") |>
    plotly::layout(
        title = "US Monthly Demand for Natural Gas",
        yaxis = list(title = "MMCF"),
        xaxis = list(title = "Month")
    )

p
```

Let's decompose the series:

```{r}
stl_d <- ts2 |>
    model(STL(y)) |>
    components()

plot_decomposition(stl_d, var = "y", outliers = TRUE)
```



Likewise, let's review the ACF plot:

```{r}
plot_acf(ts = ts2, var = "y", lag_max = 72, frequency = 12, alpha = 0.05)
```

## Add Trend Feature

We will build the features step-by-step to review the impact of each feature on the goodness of fit. Let's start by defining the trend feature:

```{r}
ts2$trend <- 1:nrow(ts2)

ts2
```

```{r}
md1 <- lm(y ~ trend, data = ts2)

summary(md1)
```


We will fit the model on the series to see the trend fit:

```{r}
fit1 <- predict(object = md1, newdata = ts2, interval = "confidence", level = 0.95)
ts2$fit1 <- fit1[, 1]
p1 <- p |>
    add_lines(x = ts2$index, y = ts2$fit1, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted")

p1
```

Let's plot the model residuals:
```{r}
#| fig-width: 15
#| fig-height: 18
plot_residuals(
    data = ts2,
    index_col = "date",
    actual_col = "y",
    fitted_col = "fit1",
    lag_max = 60,
    alpha = 0.05,
    frequency = 12
)
```


::: {.callout-tip}
## Detranding the Series

If you check the model residuals, you will noticed that the residuals are detrained version of the original series.
:::

## Add Seasonal Feature
We will now add the seasonal feature by creating a categorical variable for each month of the year:

```{r}
ts2$month <- lubridate::month(ts2$index, label = TRUE)
ts2
```


```{r}
md2 <- lm(y ~ trend + month, data = ts2)

summary(md2)
```

As you can noticed from the model summary, adding the seasonal feature improved the model goodness of fit. Let's fit the model and evaluate the model's residuals:

```{r}
fit2 <- predict(object = md2, newdata = ts2, interval = "confidence", level = 0.95)
ts2$fit2 <- fit2[, 1]
```

Let's plot the residuals:

```{r}
#| fig-width: 15
#| fig-height: 18
plot_residuals(
    data = ts2,
    index_col = "date",
    actual_col = "y",
    fitted_col = "fit2",
    lag_max = 60,
    alpha = 0.05,
    frequency = 12
)
```

Let's now create a 5 year (or 60 months) forecast by setting the future data frame:

```{r}
h <- 5 * 12
```

As before, we will use the `new_data` function from the **tsibble** library to create a continues data frame with the series future index:

```{r}
future_data <- new_data(ts2, n = h)
future_data |> head()
```

Next, let's start to add the feature we created so far - trend and seasonal features:

```{r}
# Adding the trend feature:
tail(ts2)
trend_start <- max(ts2$trend) + 1
future_data$trend <- trend_start:(trend_start + h - 1)

# Adding the seasonal feature:
future_data$month <- lubridate::month(future_data$index, label = TRUE)
future_data |> head()
```

::: {.callout-caution collapse="false"}
When creating categorical variables in a future data frame, you should ensure that the categorical variables contain the exact same levels as the ones used in the training dataset.
:::

Let's now create the forecast with the `predict` function:

```{r}
fc2 <- predict(
    object = md2,
    newdata = future_data,
    interval = "prediction",
    level = 0.95
)

future_data$yhat2 <- fc2[, 1]
future_data$lower2 <- fc2[, 2]
future_data$upper2 <- fc2[, 3]
future_data
```

```{r}
p |>
    add_lines(x = ts2$index, y = ts2$fit2, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") |>
    add_ribbons(
        x = future_data$index, ymin = future_data$lower2,
        ymax = future_data$upper2, name = "95% Prediction Interval",
        line = list(color = "rgba(7, 164, 181, 0.05)"),
        fillcolor = "rgba(7, 164, 181, 0.2)"
    ) |>
    add_lines(
        x = future_data$index,
        y = future_data$yhat2,
        name = "Forecast",
        line = list(color = "black", dash = "dash")
    ) |>
    layout(legend = list(orientation = "h", xanchor = "center", x = 0.5, y = -0.2))
```



The is a ton of information that left on the residuals!

We will explore the following features:

- Structural breaks
- Piecewise Linear Trend
- AR
- Outliers


## Structural Break Feature

Often, time series would have an abrupt shift in its structure due to some event. A good example, is the Covid-19 pandemic impact on some macro-economic indicators such as the unemployment rate and number of passengers traveling on airplanes. At the end of this change, the series would be in a new level. 

We can notice that the natural gas series had a structural break on the trend component around September 2018. To model a structural break, we will create a new variable that is equal to 0 prior to September 2018 and 1 after this date:

```{r}
ts2$structural_break <- ifelse(ts2$date >= as.Date("2018-09-01"), 1, 0)

ts2 |> head()

ts2 |> tail()
```

Let's update the regression model with the structural break feature:



```{r}
md3 <- lm(y ~ trend + month + structural_break, data = ts2)

summary(md3)
```

As you can  noticed from the model summary, adding the seasonal feature improved the model goodness of fit.


```{r}
fit3 <- predict(object = md3, newdata = ts2, interval = "confidence", level = 0.95)
ts2$fit3 <- fit3[, 1]
p3 <- p |>
    add_lines(x = ts2$index, y = ts2$fit3, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted")

p3
```

Let's plot the model residuals:
```{r}
#| fig-width: 15
#| fig-height: 18
plot_residuals(
    data = ts2,
    index_col = "date",
    actual_col = "y",
    fitted_col = "fit3",
    lag_max = 60,
    alpha = 0.05,
    frequency = 12
)
```

Let's re-forecast the series:

```{r}
future_data$structural_break <- 1
fc3 <- predict(
    object = md3,
    newdata = future_data,
    interval = "prediction",
    level = 0.95
)

future_data$yhat3 <- fc3[, 1]
future_data$lower3 <- fc3[, 2]
future_data$upper3 <- fc3[, 3]
future_data

```


```{r}
p |>
    add_lines(x = ts2$index, y = ts2$fit3, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") |>
    add_ribbons(
        x = future_data$index, ymin = future_data$lower3,
        ymax = future_data$upper3, name = "95% Prediction Interval",
        line = list(color = "rgba(7, 164, 181, 0.05)"),
        fillcolor = "rgba(7, 164, 181, 0.2)"
    ) |>
    add_lines(
        x = future_data$index,
        y = future_data$yhat3,
        name = "Forecast",
        line = list(color = "black", dash = "dash")
    ) |>
    layout(legend = list(orientation = "h", xanchor = "center", x = 0.5, y = -0.2))
```







## Forecast Simulation Using Coefficient Uncertainty

The simulation-based forecasting approach accounts for uncertainty in model parameters by treating each estimated coefficient as a random variable rather than a fixed value. When fitting a linear regression model, we obtain not only point estimates for each coefficient but also their standard errors, which quantify the uncertainty in those estimates. Under standard regression assumptions, each coefficient follows a normal distribution centered at its estimated value with a spread determined by its standard error. For each simulation path, we randomly draw a complete set of coefficients from these normal distributions: $\beta_i \sim N(\hat{\beta}_i, SE(\hat{\beta}_i))$, where $\hat{\beta}_i$ is the estimated coefficient and $SE(\hat{\beta}_i)$ is its standard error. This creates a plausible alternative version of the model that reflects our uncertainty about the true parameter values.

Once we have a set of simulated coefficients, we apply them to the future predictor values to generate forecasts. For each time step in the forecast horizon, we calculate the prediction as $\hat{y}_t = X_t \times \beta_{sim}$, where $X_t$ is the row of the model matrix containing the predictor values (including the intercept, trend, dummy variables, etc.) and $\beta_{sim}$ is our vector of simulated coefficients. If residual error is included (controlled by the `add_residual_error` parameter), we also add a random draw from $N(0, \sigma)$, where $\sigma$ is the model's residual standard error, capturing the irreducible random variation in the data. For models with lagged dependent variables (AR models), the process becomes recursive: the prediction for time $t$ is used as the lagged value for time $t+1$, propagating both coefficient uncertainty and random error through the forecast horizon. By repeating this process many times (typically 1,000+ simulations), we build up a distribution of possible future outcomes, which can be visualized as a "cloud" of paths and summarized using prediction intervals.



```{r}
sim <- sim_forecast(
    model = md3,
    future_data = future_data,
    n_sims = 50,
    seed = 1234
)
```


```{r}
plot_sim_forecast(
    actual_data = ts2,
    sim_data = sim,
    index_col = "index",
    actual_col = "y"
)
```